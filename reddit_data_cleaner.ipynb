{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSlt6z7Tga0eC74XP0kzRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahilGhg/Social-Media-Analytics/blob/main/reddit_data_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a2GS_8Gi-db",
        "outputId": "d85db12d-9d5c-43c4-f35c-065eb12c3186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordsegment\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.9.4-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2025.8.3)\n",
            "Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_tool_python-2.9.4-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wordsegment, language-tool-python\n",
            "Successfully installed language-tool-python-2.9.4 wordsegment-1.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wordsegment language-tool-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import wordsegment\n",
        "from textblob import TextBlob\n",
        "import language_tool_python\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# lang_tool = language_tool_python.LanguageTool('en-US') # Uncomment for grammar correction\n",
        "\n",
        "def clean_social_media_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    A single function to clean a DataFrame containing social media text data.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Cleaning Process ---\")\n",
        "\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    # Step 0: Filter out deleted/removed entries\n",
        "    original_rows = len(clean_df)\n",
        "    # The `isin()` method checks for exact matches to '[deleted]' or '[removed]'\n",
        "    # The `~` symbol inverts the selection, keeping all rows that DO NOT match.\n",
        "    clean_df = clean_df[~clean_df['text'].isin(['[deleted]', '[removed]'])]\n",
        "    rows_removed = original_rows - len(clean_df)\n",
        "    if rows_removed > 0:\n",
        "        print(f\"Step 0: Removed {rows_removed} deleted/removed entries.\")\n",
        "\n",
        "    # Step 1: Duplicate Removal\n",
        "    clean_df.drop_duplicates(inplace=True)\n",
        "    print(\"Step 1: Duplicates removed.\")\n",
        "\n",
        "    text_column = clean_df['text']\n",
        "\n",
        "    # Step 2: Convert to Lowercase\n",
        "    text_column = text_column.str.lower()\n",
        "    print(\"Step 2: Converted text to lowercase.\")\n",
        "\n",
        "    # Step 3: Remove URLs\n",
        "    text_column = text_column.apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
        "    print(\"Step 3: URLs removed.\")\n",
        "\n",
        "    # Step 4: Remove HTML tags\n",
        "    text_column = text_column.apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
        "    print(\"Step 4: HTML tags removed.\")\n",
        "\n",
        "    # Step 5: Basic Cleaning (Remove mentions, hashtags, and special characters)\n",
        "    text_column = text_column.apply(lambda x: re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', x))\n",
        "    text_column = text_column.apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "    print(\"Step 5: Mentions, hashtags, and special chars removed.\")\n",
        "\n",
        "    # Step 6: Remove Multiple Letters (e.g., 'sooo' -> 'so')\n",
        "    text_column = text_column.apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x))\n",
        "    print(\"Step 6: Elongated words shortened\")\n",
        "\n",
        "    # Step 7: Whitespace Removal\n",
        "    text_column = text_column.apply(lambda x: x.strip())\n",
        "    text_column = text_column.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "    print(\"Step 7: Extra whitespace removed.\")\n",
        "\n",
        "    # Step 8: Split Attached Words (e.g., 'goodservice' -> 'good service')\n",
        "    # text_column = text_column.apply(lambda x: ' '.join(wordsegment.segment(x)))\n",
        "    # print(\"Step 8: Attached words split.\")\n",
        "\n",
        "    # # Step 9: Spelling Correction\n",
        "    # text_column = text_column.apply(lambda x: str(TextBlob(x).correct()))\n",
        "    # print(\"Step 9: Spelling correction applied.\")\n",
        "\n",
        "    # Step 10: Grammar Correction\n",
        "    # text_column = text_column.apply(lambda x: lang_tool.correct(x))\n",
        "    # print(\"Step 10: Grammar correction applied.\")\n",
        "\n",
        "    # Step 11: Tokenization\n",
        "    text_column = text_column.apply(word_tokenize)\n",
        "    print(\"Step 11: Text tokenized.\")\n",
        "\n",
        "    # Step 12: Remove Stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    custom_stopwords = ['zomato', 'title', 'body']\n",
        "    stop_words.update(custom_stopwords)\n",
        "    text_column = text_column.apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "    print(\"Step 12: Stopwords removed.\")\n",
        "\n",
        "    # Step 13: Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text_column = text_column.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "    print(\"Step 13: Words lemmatized.\")\n",
        "\n",
        "    # Assign the cleaned text back to the DataFrame\n",
        "    clean_df['cleaned_text_tokens'] = text_column\n",
        "\n",
        "    print(\"--- Cleaning Process Finished ---\")\n",
        "    return clean_df"
      ],
      "metadata": {
        "id": "ABl0a8WejZdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07e982c-1db7-4b83-a016-cc7058d888ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. LOAD**"
      ],
      "metadata": {
        "id": "ZXci0VSXj6hD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the raw data you collected in Experiment 2"
      ],
      "metadata": {
        "id": "XikH4ptdkDOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_path = 'reddit_rawdata.csv'\n",
        "raw_df = pd.read_csv(raw_data_path)\n",
        "print(\"Raw data loaded successfully.\")\n",
        "print(f\"Shape of raw data: {raw_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA7Jxz2yjsqE",
        "outputId": "1f1216f8-dcd0-425a-c6a7-6771d7f2d688"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data loaded successfully.\n",
            "Shape of raw data: (1232, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. CLEAN**"
      ],
      "metadata": {
        "id": "UF6_cd1ukKaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = clean_social_media_data(raw_df)\n",
        "print(\"\\nData has been cleaned.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiQlrQk_kWHS",
        "outputId": "73172d75-add3-4a58-f97f-206c41bdf99f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Cleaning Process ---\n",
            "Step 0: Removed 44 deleted/removed entries.\n",
            "Step 1: Duplicates removed.\n",
            "Step 2: Converted text to lowercase.\n",
            "Step 3: URLs removed.\n",
            "Step 4: HTML tags removed.\n",
            "Step 5: Mentions, hashtags, and special chars removed.\n",
            "Step 6: Elongated words shortened\n",
            "Step 7: Extra whitespace removed.\n",
            "Step 11: Text tokenized.\n",
            "Step 12: Stopwords removed.\n",
            "Step 13: Words lemmatized.\n",
            "--- Cleaning Process Finished ---\n",
            "\n",
            "Data has been cleaned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. SAVE**"
      ],
      "metadata": {
        "id": "D-U5MrFyX8uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data_path = 'reddit_data_cleaned_2.csv'\n",
        "cleaned_df.to_csv(cleaned_data_path, index=False)\n",
        "print(f\"Cleaned data saved to '{cleaned_data_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGg7MB6bX7Yj",
        "outputId": "bbe777e6-5b23-411d-e833-eb85773c886a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'reddit_data_cleaned_2.csv'\n"
          ]
        }
      ]
    }
  ]
}